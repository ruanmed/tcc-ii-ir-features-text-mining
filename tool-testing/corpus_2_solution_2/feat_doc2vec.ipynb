{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2019 Hyperpartisan News Detection\n",
    "#### Using Doc2Vec as document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from lxml.etree import iterparse\n",
    "import xml\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import SaveLoad, simple_preprocess\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterCorpus():\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "    def __iter__(self):\n",
    "        for event, elem in iterparse(self.file):\n",
    "            if elem.tag == \"article\":\n",
    "                articleId = elem.attrib['id']\n",
    "                title = elem.attrib['title']\n",
    "                text = \"\".join(elem.itertext())\n",
    "                text = textCleaning(title, text)             \n",
    "                yield(articleId, text)\n",
    "                \n",
    "                \n",
    "class TaggedDoc(object):\n",
    "    '''\n",
    "    prepare tagged documents for the doc2vec model\n",
    "    '''\n",
    "    def __init__(self, file):\n",
    "        if isinstance(file, str):\n",
    "            self.file = [file]\n",
    "        else:\n",
    "            self.file = file\n",
    "\n",
    "    def __iter__(self):\n",
    "        for f in self.file:\n",
    "            if 'byarticle' in f:\n",
    "                fileIdx = 2\n",
    "            elif 'validation' in f:\n",
    "                fileIdx = 1\n",
    "            else:\n",
    "                fileIdx = 0\n",
    "            corpus = IterCorpus(f)\n",
    "            for text in corpus:\n",
    "                ind = text[0] + '_' + str(fileIdx)\n",
    "                yield TaggedDocument(simple_preprocess(text[1]), [ind])\n",
    "            \n",
    "            \n",
    "def extract_doc_rep(textFile, model):\n",
    "    '''\n",
    "    Extract the representation for file docFile\n",
    "    '''\n",
    "    vectors = np.zeros((400, 645))\n",
    "    cnt = 0\n",
    "    corpus = TaggedDoc(textFile)\n",
    "    for doc in corpus:\n",
    "        vectors[:, cnt] = model.infer_vector(doc.words, epochs=100, alpha=0.025)\n",
    "        cnt = cnt+1\n",
    "    return vectors\n",
    "                        \n",
    "def readLabels(labelFile):\n",
    "    y = []\n",
    "    with open(labelFile) as labelFile:\n",
    "        xml.sax.parse(labelFile, GroundTruthHandler(y))\n",
    "       \n",
    "    return np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for data\n",
    "dataPath = 'data/'\n",
    "modelPath = \"tmp/\"\n",
    "textFile = dataPath + 'articles-training-byarticle.xml'\n",
    "labelFile = dataPath + \"ground-truth-training-byarticle.xml\"\n",
    "labels = readLabels(labelFile)\n",
    "\n",
    "# split the samples with the same seed to compare results with other methods\n",
    "id1, id2 = fixedTestSplit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model = Doc2Vec.load(modelPath + \"doc2vec_400_300\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 0.7739475965499878), ('google', 0.7013559341430664), ('tesla', 0.6806751489639282), ('microsoft', 0.6742485761642456), ('netflix', 0.6667925715446472), ('boeing', 0.652466356754303), ('china', 0.6521828174591064), ('it', 0.6418113112449646), ('intel', 0.6391427516937256), ('he', 0.634005069732666)]\n",
      "[('obama', 0.8785072565078735), ('bush', 0.8242968320846558), ('clinton', 0.7938897609710693), ('romney', 0.785071849822998), ('he', 0.766777515411377), ('sanders', 0.7613655924797058), ('pence', 0.7346415519714355), ('putin', 0.7339059114456177), ('the', 0.7321258783340454), ('she', 0.7216925024986267)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# explore a few words and their similar words\n",
    "print(word_vectors.similar_by_word(\"amazon\"))\n",
    "print(word_vectors.similar_by_word(\"trump\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = extract_doc_rep(textFile, model)\n",
    "vectors = vectors.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = vectors[id1,:]\n",
    "testX = vectors[id2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KernelSVM] C=0.500000 | acc=0.639706\n",
      "[KernelSVM] C=1.000000 | acc=0.739168\n",
      "[KernelSVM] C=3.000000 | acc=0.776378\n",
      "[KernelSVM] C=5.000000 | acc=0.779503\n",
      "[KernelSVM] C=10.000000 | acc=0.779503\n"
     ]
    }
   ],
   "source": [
    "C = [0.5, 1,3, 5, 10]\n",
    "for c in C:\n",
    "    svm = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=c, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "    print(\"[KernelSVM] C=%f | acc=%f\" %(c,np.mean(cross_val_score(svm, trainX, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticR] C=0.001000 | acc=0.729875\n",
      "[LogisticR] C=0.050000 | acc=0.723341\n",
      "[LogisticR] C=0.070000 | acc=0.723247\n",
      "[LogisticR] C=0.100000 | acc=0.717186\n",
      "[LogisticR] C=0.500000 | acc=0.707994\n",
      "[LogisticR] C=1.000000 | acc=0.708095\n"
     ]
    }
   ],
   "source": [
    "C = [0.001, 0.05, 0.07, 0.1, 0.5, 1]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(solver = 'lbfgs', C = c, max_iter=5000)\n",
    "    print(\"[LogisticR] C=%f | acc=%f\" %(c,np.mean(cross_val_score(lr, trainX, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.7894736842105263\n",
      "Test precision:  0.7802197802197802\n",
      "Test recall:  0.5966386554621849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[184,  20],\n",
       "       [ 48,  71]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the classifier that has highest cv accuracy as the final model\n",
    "model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=5, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "#model = LogisticRegression(solver = 'lbfgs', C = 0.05, max_iter=1000)\n",
    "model.fit(trainX, labels[id1])\n",
    "trn_pred = model.predict(trainX)\n",
    "tst_pred = model.predict(testX)\n",
    "print('Train accuracy: ', accuracy_score(labels[id1], trn_pred))\n",
    "print('Test accuracy: ', accuracy_score(labels[id2], tst_pred))\n",
    "print('Test precision: ', precision_score(labels[id2], tst_pred, pos_label='true'))\n",
    "print('Test recall: ', recall_score(labels[id2], tst_pred, pos_label='true'))\n",
    "confusion_matrix(labels[id2], tst_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model to all samples\n",
    "model.fit(vectors, labels)\n",
    "# save the model\n",
    "pickle.dump(model, open('trained_clsf/svm_doc2vec.sav', 'wb'))\n",
    "# save the predictions\n",
    "np.save(\"predictions/doc2vec_svm_pred\", tst_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to stack features to doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load extracted features\n",
    "features = np.load(\"features.npy\")\n",
    "final = np.hstack((vectors, features))\n",
    "trainX = final[id1]\n",
    "testX = final[id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KernelSVM] C=0.100000 | acc=0.630425\n",
      "[KernelSVM] C=0.500000 | acc=0.729875\n",
      "[KernelSVM] C=1.000000 | acc=0.764067\n",
      "[KernelSVM] C=3.000000 | acc=0.763688\n",
      "[KernelSVM] C=5.000000 | acc=0.763688\n",
      "[KernelSVM] C=10.000000 | acc=0.763688\n"
     ]
    }
   ],
   "source": [
    "C = [0.1, 0.5, 1,3, 5, 10]\n",
    "for c in C:\n",
    "    svm = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=c, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "    print(\"[KernelSVM] C=%f | acc=%f\" %(c,np.mean(cross_val_score(svm, trainX, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticR] C=0.001000 | acc=0.729875\n",
      "[LogisticR] C=0.005000 | acc=0.748448\n",
      "[LogisticR] C=0.050000 | acc=0.723341\n",
      "[LogisticR] C=0.100000 | acc=0.717186\n"
     ]
    }
   ],
   "source": [
    "C = [0.001, 0.005, 0.05, 0.1]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(solver = 'lbfgs', C = c, max_iter=5000)\n",
    "    print(\"[LogisticR] C=%f | acc=%f\" %(c,np.mean(cross_val_score(lr, trainX, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9316770186335404\n",
      "Test accuracy:  0.7770897832817337\n",
      "Test precision:  0.7422680412371134\n",
      "Test recall:  0.6050420168067226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[179,  25],\n",
       "       [ 47,  72]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the classifier that has highest cv accuracy as the final model\n",
    "model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=1, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "model = LogisticRegression(solver = 'lbfgs', C = 0.005, max_iter=1000)\n",
    "model.fit(trainX, labels[id1])\n",
    "trn_pred = model.predict(trainX)\n",
    "tst_pred = model.predict(testX)\n",
    "print('Train accuracy: ', accuracy_score(labels[id1], trn_pred))\n",
    "print('Test accuracy: ', accuracy_score(labels[id2], tst_pred))\n",
    "print('Test precision: ', precision_score(labels[id2], tst_pred, pos_label='true'))\n",
    "print('Test recall: ', recall_score(labels[id2], tst_pred, pos_label='true'))\n",
    "confusion_matrix(labels[id2], tst_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

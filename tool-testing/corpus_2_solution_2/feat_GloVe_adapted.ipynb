{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemEval2019 Hyperpartisan News Detection\n",
    "#### Using GloVe as document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml.etree import iterparse\n",
    "import xml\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFiles(textFile, labelFile):\n",
    "    X,y = [], []    \n",
    "    with open(labelFile) as labelFile:\n",
    "        xml.sax.parse(labelFile, GroundTruthHandler(y))\n",
    "       \n",
    "    for event, elem in iterparse(textFile):\n",
    "        if elem.tag == \"article\":\n",
    "            title = elem.attrib['title']\n",
    "            text = \"\".join(elem.itertext())\n",
    "            title = cleanQuotations(title)\n",
    "            text = cleanQuotations(text)\n",
    "            text = cleanText(fixup(text))\n",
    "            text = ' '.join(text.split()[:1000])\n",
    "            X.append(title + \". \" + text)\n",
    "            elem.clear()\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove(path, dim):\n",
    "    '''\n",
    "    read the glove vectors from path with dimension dim\n",
    "    '''\n",
    "    df = pd.read_csv(path + 'glove.6B.' + str(dim) + 'd.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove = {key: val.values for key, val in df.T.items()}\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set path for data\n",
    "dataPath = 'data/'\n",
    "pretrained_wv_path = \"pretrained_wv/\"\n",
    "\n",
    "textFile = dataPath + 'articles-training-byarticle.xml'\n",
    "labelFile = dataPath + \"ground-truth-training-byarticle.xml\"\n",
    "\n",
    "# read in data and glove vectors\n",
    "texts, labels = readFiles(textFile, labelFile)\n",
    "glove = read_glove(pretrained_wv_path, 300)\n",
    "\n",
    "# split the samples with the same seed to compare results with other methods\n",
    "id1, id2 = fixedTestSplit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gloveVectorize(glove, text):\n",
    "    '''\n",
    "    Find the pretrained glove vectors of the first 1000 words in the articles.\n",
    "    The final vector is the average of the vectors\n",
    "    '''\n",
    "    dim = len(glove[\"the\"])\n",
    "    X = np.zeros( (len(text), dim) )\n",
    "    for text_id, t in enumerate(text):\n",
    "        tmp = np.zeros((1, dim))\n",
    "        \n",
    "        # tokenize and remove stopwords\n",
    "        words = customTokenize(t, rm_stopwords=True)\n",
    "        words = [w for w in words if w in glove.keys()]\n",
    "        for word in words:\n",
    "            tmp[:] += glove[word]\n",
    "        X[text_id, :] = tmp/len(words)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_texts = gloveVectorize(glove, texts)\n",
    "train_x = glove_texts[id1]\n",
    "test_x = glove_texts[id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KernelSVM] C=0.500000 | acc=0.764067\n",
      "[KernelSVM] C=0.600000 | acc=0.776472\n",
      "[KernelSVM] C=0.700000 | acc=0.776567\n",
      "[KernelSVM] C=0.900000 | acc=0.779793\n",
      "[KernelSVM] C=1.000000 | acc=0.773442\n",
      "[KernelSVM] C=1.100000 | acc=0.776472\n",
      "[KernelSVM] C=1.200000 | acc=0.776472\n",
      "[KernelSVM] C=5.000000 | acc=0.751176\n",
      "[KernelSVM] C=10.000000 | acc=0.751075\n"
     ]
    }
   ],
   "source": [
    "C = [0.5, 0.6, 0.7, 0.9,1,1.1, 1.2, 5,10]\n",
    "for c in C:\n",
    "    kernel_svm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(C=c, gamma=\"auto\", max_iter = 1000))\n",
    "    ])\n",
    "    print(\"[KernelSVM] C=%f | acc=%f\" %(c,np.mean(cross_val_score(kernel_svm, train_x, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticR] C=0.050000 | acc=0.645760\n",
      "[LogisticR] C=0.100000 | acc=0.693038\n",
      "[LogisticR] C=0.500000 | acc=0.748714\n",
      "[LogisticR] C=0.800000 | acc=0.757805\n",
      "[LogisticR] C=0.900000 | acc=0.764061\n",
      "[LogisticR] C=1.000000 | acc=0.764061\n",
      "[LogisticR] C=2.000000 | acc=0.751750\n",
      "[LogisticR] C=3.000000 | acc=0.748525\n",
      "[LogisticR] C=5.000000 | acc=0.745400\n"
     ]
    }
   ],
   "source": [
    "C = [0.05, 0.1, 0.5, 0.8, 0.9, 1, 2, 3, 5]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(solver = 'lbfgs', C = c, max_iter=1000)\n",
    "    print(\"[LogisticR] C=%f | acc=%f\" %(c,np.mean(cross_val_score(lr, train_x, labels[id1], cv=10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9130434782608695\n",
      "Test accuracy:  0.7956656346749226\n",
      "Test precision:  0.7676767676767676\n",
      "Test recall:  0.6386554621848739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[181,  23],\n",
       "       [ 43,  76]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svc\", SVC(C=0.9, gamma=\"auto\", max_iter = 5000))\n",
    "        ])\n",
    "\n",
    "# model = LogisticRegression(solver = 'lbfgs', C = 1, max_iter=1000)\n",
    "model.fit(train_x, labels[id1])\n",
    "trn_pred = model.predict(train_x)\n",
    "tst_pred = model.predict(test_x)\n",
    "print('Train accuracy: ', accuracy_score(labels[id1], trn_pred))\n",
    "print('Test accuracy: ', accuracy_score(labels[id2], tst_pred))\n",
    "print('Test precision: ', precision_score(labels[id2], tst_pred, pos_label='true'))\n",
    "print('Test recall: ', recall_score(labels[id2], tst_pred, pos_label='true'))\n",
    "confusion_matrix(labels[id2], tst_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model to all samples\n",
    "model.fit(glove_texts, labels)\n",
    "# save the model\n",
    "pickle.dump(model, open('trained_clsf/svm_glove.sav', 'wb'))\n",
    "# save the predictions\n",
    "np.save(\"predictions/glove_svm_pred\", tst_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
